infrastructure:
  enabled: true
  
  # Database configurations
  postgres:
    enabled: true
    image:
      repository: postgres
      tag: 16-alpine
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
      POSTGRES_MULTIPLE_DATABASES: customer,product,vendor,payment
    persistence:
      enabled: true
      size: 10Gi
    resources:
      limits:
        cpu: 1
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 512Mi

  mongodb:
    enabled: true
    image:
      repository: mongo
      tag: "7.0"
    service:
      type: ClusterIP
      port: 27017
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: admin
    persistence:
      enabled: true
      size: 10Gi
    resources:
      limits:
        cpu: 1
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 512Mi


  redis:
    enabled: true
    image:
      repository: redis
      tag: "7-alpine"
    command: ["redis-server", "--requirepass", "redis"]
    service:
      type: ClusterIP
      port: 6379
    persistence:
      enabled: true
      size: 5Gi
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 250m
        memory: 256Mi

  # Messaging configurations
  zookeeper:
    enabled: true
    service:
      type: ClusterIP
      port: 2181
    image:
      repository: confluentinc/cp-zookeeper
      tag: "7.5.0"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    persistence:
      dataDirSize: 5Gi
      logDirSize: 2Gi
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 250m
        memory: 512Mi

  kafka:
    enabled: true
    image:
      repository: confluentinc/cp-kafka
      tag: "7.5.0"
    service:
      type: ClusterIP
      ports:
        - port: 9092
          targetPort: kafka
          name: kafka
        - port: 29092
          targetPort: kafka-host
          name: kafka-host
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "{{ .Release.Name }}-zookeeper:2181"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://{{ .Release.Name }}-kafka:9092,PLAINTEXT_HOST://localhost:29092"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT"
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_DELETE_TOPIC_ENABLE: "true"
    persistence:
      enabled: true
      size: 10Gi
    resources:
      limits:
        cpu: 1
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi

  # Observability stack
  jaeger:
    enabled: true
    image:
      repository: jaegertracing/all-in-one
      tag: latest
    service:
      type: ClusterIP
      ports:
        - port: 16686
          targetPort: 16686
          name: ui
        - port: 14250
          targetPort: 14250
          name: grpc
        - port: 14268
          targetPort: 14268
          name: http
        - port: 4317
          targetPort: 4317
          name: otlp-grpc
        - port: 4318
          targetPort: 4318
          name: otlp-http
        - port: 9411
          targetPort: 9411
          name: zipkin
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - COLLECTOR_ZIPKIN_HOST_PORT=9411
    resources:
      limits:
        cpu: 1
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 512Mi

  prometheus:
    enabled: true
    image:
      repository: prom/prometheus
      tag: "v2.40.0"
    service:
      type: ClusterIP
      port: 9090
    persistence:
      enabled: true
      size: 10Gi
    resources:
      limits:
        cpu: "1"
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 512Mi
    configMap:
      name: prometheus-config
      data:
        prometheus.yml: |
          global:
            scrape_interval: 15s
            evaluation_interval: 15s
          scrape_configs:
            - job_name: "prometheus"
              static_configs:
                - targets: ["localhost:9090"]

  otel-collector:
    enabled: true
    image:
      repository: otel/opentelemetry-collector-contrib
      tag: latest
    configMap:
      name: otel-collector-config
      data:
        otel-collector-config.yaml: |
          receivers:
            otlp:
              protocols:
                grpc:
                  endpoint: 0.0.0.0:4317
                http:
                  endpoint: 0.0.0.0:4318
          
          processors:
            batch:
              timeout: 1s
              send_batch_size: 1024
          
          exporters:
            otlp:
              endpoint: "{{ .Release.Name }}-jaeger:4317"
              tls:
                insecure: true
            prometheus:
              endpoint: 0.0.0.0:8889
          
          service:
            pipelines:
              traces:
                receivers: [otlp]
                processors: [batch]
                exporters: [otlp]
              metrics:
                receivers: [otlp]
                processors: [batch]
                exporters: [prometheus]
    service:
      type: ClusterIP
      ports:
        - port: 4317
          targetPort: grpc
          name: grpc
        - port: 4318
          targetPort: http
          name: http
        - port: 8889
          targetPort: prometheus
          name: prometheus
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 250m
        memory: 256Mi

  logstash:
    enabled: true
    image:
      repository: docker.elastic.co/logstash/logstash
      tag: "8.9.0"
    configMap:
      name: logstash-config
      data:
        # JVM options
        jvm.options: |
          -Xms256m
          -Xmx256m
        log4j2.properties: |
          status = error
          name = LogstashPropertiesConfig
          appender.console.type = Console
          appender.console.name = plain_console
          appender.console.layout.type = PatternLayout
          appender.console.layout.pattern = "[%d{ISO8601}][%-5p][%-25c]%notEmpty{[%X{pipeline.id}]}%notEmpty{[%X{plugin.id}]} %m%n"
          rootLogger.level = ${sys:ls.log.level}
          rootLogger.appenderRef.console.ref = plain_console
        logstash.yml: |
          http.host: "0.0.0.0"
          xpack.monitoring.enabled: false
        pipelines.yml: |
          - pipeline.id: main
            path.config: "/usr/share/logstash/pipeline/logstash.conf"
        logstash.conf: |
          input {
            tcp {
              port => 5000
              host => "0.0.0.0"
              codec => json
            }
          }
          
          filter {
            if [environment] {
              mutate { add_tag => [ "%{environment}" ] }
            }
            if [application] {
              mutate { add_tag => [ "%{application}" ] }
            }
          }
          
          output {
            elasticsearch {
              hosts => ["elasticsearch:9200"]
              index => "app-logs-%{+YYYY.MM.dd}"
              manage_template => true
              template_overwrite => true
            }
            stdout { codec => rubydebug }
          }
    service:
      type: ClusterIP
      ports:
        - name: tcp
          port: 5000
          targetPort: tcp
        - name: api
          port: 9600
          targetPort: api
    environment:
      - name: LS_JAVA_OPTS
        value: "-Xms256m -Xmx512m"
    resources:
      limits:
        cpu: "1"
        memory: "1Gi"
      requests:
        cpu: "500m"
        memory: "512Mi"

  elasticsearch:
    enabled: true
    image:
      repository: docker.elastic.co/elasticsearch/elasticsearch
      tag: "8.9.0"
    service:
      type: ClusterIP
      port: 9200
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - xpack.security.enabled=false
      - xpack.monitoring.collection.enabled=true
    persistence:
      enabled: true
      size: 10Gi
    resources:
      limits:
        cpu: 1
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi

  kibana:
    enabled: true
    image:
      repository: docker.elastic.co/kibana/kibana
      tag: "8.9.0"
    service:
      type: ClusterIP
      port: 5601
    environment:
      ELASTICSEARCH_HOSTS: http://{{ .Release.Name }}-elasticsearch:9200
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 250m
        memory: 512Mi

  grafana:
    enabled: true
    image:
      repository: grafana/grafana
      tag: "10.0.0"
    service:
      type: ClusterIP
      port: 3000
    configMap:
      name: grafana-datasources
      data:
        datasources.yaml: |
          apiVersion: 1
          datasources:
          - name: Prometheus
            type: prometheus
            url: http://{{ .Release.Name }}-prometheus:9090
            access: proxy
            isDefault: true
          - name: Jaeger
            type: jaeger
            url: http://{{ .Release.Name }}-jaeger:16686
            access: proxy
    environment:
      - name: GF_SECURITY_ADMIN_USER
        value: admin
      - name: GF_SECURITY_ADMIN_PASSWORD
        value: admin
      - name: GF_USERS_ALLOW_SIGN_UP
        value: "false"
    persistence:
      enabled: true
      size: 5Gi
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 250m
        memory: 256Mi

  alertmanager:
    enabled: true
    image:
      repository: prom/alertmanager
      tag: "v0.25.0"
    service:
      type: ClusterIP
      port: 9093
    configMap:
      name: alertmanager-config
      data:
        alertmanager.yml: |
          global:
            resolve_timeout: 5m
            smtp_smarthost: 'smtp.example.org:587'
            smtp_from: 'alertmanager@example.org'
            smtp_auth_username: 'alertmanager'
            smtp_auth_password: 'password'
            smtp_require_tls: true

          route:
            group_by: ['alertname', 'job']
            group_wait: 30s
            group_interval: 5m
            repeat_interval: 12h
            receiver: 'team-emails'
            routes:
            - match:
                severity: critical
              receiver: 'team-emails'

          receivers:
          - name: 'team-emails'
            email_configs:
            - to: 'team@example.org'
              send_resolved: true

          inhibit_rules:
          - source_match:
              severity: 'critical'
            target_match:
              severity: 'warning'
            equal: ['alertname', 'dev', 'instance']
    persistence:
      size: 5Gi
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 128Mi